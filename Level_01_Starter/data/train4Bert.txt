RNN（Recurrent Neural Network）即循环神经网络，<<SEP>>这是一系列神经网络的统称，<<SEP>>其特点是在序列数据上重复使用相同的结构，<<SEP>>对序列数据中的依存关系进行建模，<<SEP>>用于对序列的预测<<SEP>>
先简单的看一下这种网络，<<SEP>>这种网络的做法是：在序列数据的每一个位置重复使用相同的前馈神经网络，<<SEP>>并将相邻位置的神经网络连接起来；<<SEP>>用于前馈神经网络隐层的输出表示当前位置的“状态”ht，<<SEP>>输入的token经过word embedding变成词向量Xt，<<SEP>>词向量Xt输入RNN会得到一个当前位置的“状态”ht，<<SEP>>其中A是事先随机生成的参数矩阵，<<SEP>>可以在训练过程中不断学习调整参数，<<SEP>>当前位置的状态依赖于当前位置的词向量输入和来自前面所有状态的输入。<<SEP>>
这种网络的做法有意在模仿人脑的记忆功能，<<SEP>>比如我们阅读一个词就会记住这个词的信息，<<SEP>>在阅读后面的词时就会结合前面阅读到的词的信息<<SEP>>
Simple RNN有什么缺点呢？<<SEP>>
Simple RNN不适合长距离”记忆“，<<SEP>>它处理短距离依赖关系的能力很强但是长距离”记忆“能力不行；<<SEP>>例如做预测任务时，<<SEP>>输入到fluent时我们要预测Chinese，<<SEP>>因为前文已经出现了China，<<SEP>>但是Simple RNN可能已经”忘记“了前面出现过的China，<<SEP>>所以不能预测出正确结果
2013- 词嵌入<<SEP>>
用稀疏向量表示文本，<<SEP>>即所谓的词袋模型在 NLP 有着悠久的历史。<<SEP>>正如上文中介绍的，<<SEP>>早在 2001年就开始使用密集向量表示词或词嵌入。<<SEP>>Mikolov等人在2013年提出的创新技术是通过去除隐藏层，<<SEP>>逼近目标，<<SEP>>进而使这些单词嵌入的训练更加高效。<<SEP>>虽然这些技术变更本质上很简单，<<SEP>>但它们与高效的word2vec配合使用，<<SEP>>便能使大规模的词嵌入训练成为可能。<<SEP>>
Word2vec有两种风格，<<SEP>>如下面的图所示：连续字袋 CBOW 和 skip-gram。<<SEP>>不过他们的目标不同：一个是根据周围的单词预测中心单词，<<SEP>>而另一个则相反。<<SEP>>
虽然这些嵌入在概念上与使用前馈神经网络学习的嵌入在概念上没有区别，<<SEP>>但是在一个非常大的语料库上训练之后，<<SEP>>它们就能够捕获诸如性别、动词时态和国家-首都关系等单词之间的特定关系。<<SEP>>
2013 - NLP 神经网络<<SEP>>
2013 年和 2014 年是 NLP 问题开始引入神经网络模型的时期。<<SEP>>使用最广泛的三种主要的神经网络是：循环神经网络、卷积神经网络和递归神经网络。<<SEP>>
循环神经网络（RNNs） 循环神经网络是处理 NLP 中普遍存在的动态输入序列的一个最佳的技术方案。<<SEP>>Vanilla RNNs （Elman,1990）很快被经典的长-短期记忆网络（Hochreiter & Schmidhuber,1997）所取代，<<SEP>>它被证明对消失和爆炸梯度问题更有弹性。<<SEP>>在 2013 年之前，<<SEP>>RNN 仍被认为很难训练；Ilya Sutskever 的博士论文为改变这种现状提供了一个关键性的例子。<<SEP>>下面的图对 LSTM 单元进行了可视化显示。<<SEP>>双向 LSTM（Graves等,2013）通常用于处理左右两边的上下文。<<SEP>>
2014-sequence-to-sequence 模型<<SEP>>
2014 年，<<SEP>>Sutskever 等人提出了 sequence-to-sequence 模型。<<SEP>>这是一个使用神经网络将一个序列映射到另一个序列的通用框架。<<SEP>>在该框架中，<<SEP>>编码器神经网络逐符号处理一个句子，<<SEP>>并将其压缩为一个向量表示；然后，<<SEP>>一个解码器神经网络根据编码器状态逐符号输出预测值，<<SEP>>并将之前预测的符号作为每一步的输入。<<SEP>>