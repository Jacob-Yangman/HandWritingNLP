1、什么是bert<<SEP>>
BERT（Bidirectional Encoder Representations from Transformers）是一个基于Transformer架构的双向编码器表示模型，<<SEP>>它通过预训练学习到了丰富的语言表示，<<SEP>>并可以用于各种自然语言处理任务。<<SEP>>
2、模型结构<<SEP>>
BERT基于Transformer的编码器部分，<<SEP>>采用了多层自注意力机制和前馈神经网络。<<SEP>>这种结构使得BERT能够同时考虑文本中的上下文信息，<<SEP>>从而捕捉到更加丰富的语义特征。<<SEP>>
自注意力机制：<<SEP>>
自注意力机制，<<SEP>>是一种让模型在处理输入数据时能够自我关注并捕捉序列内部元素之间关系的机制。<<SEP>>与传统的注意力机制不同，<<SEP>>自注意力机制不依赖于外部信息，<<SEP>>而是直接分析序列内部的相互依赖性。<<SEP>>它通过对序列中的每个元素计算它与其他所有元素之间的关联性，<<SEP>>并据此生成一个加权向量，<<SEP>>这个加权向量可以看作是该元素在当前上下文中的表示。<<SEP>>
3、预训练任务<<SEP>>
BERT通过两个无监督的预测任务进行预训练，<<SEP>>即遮蔽语言模型（Masked Language Model, MLM）和下一句预测（Next Sentence Prediction, NSP）。<<SEP>>在MLM任务中，<<SEP>>模型需要预测被遮蔽的词；在NSP任务中，<<SEP>>模型需要判断两个句子是否是连续的。<<SEP>>这两个任务使得BERT能够学习到语言的深层结构和语义信息。<<SEP>>
4、双向性<<SEP>>
与之前的语言模型（如GPT）主要依赖之前或之后的上下文不同，<<SEP>>BERT是双向的。<<SEP>>这意味着它在预测一个词时会同时考虑该词前后的上下文，<<SEP>>从而更准确地捕捉语义信息。<<SEP>>
5、微调（Fine-tuning）<<SEP>>
在完成预训练后，<<SEP>>BERT可以通过微调来适应各种下游任务。<<SEP>>微调是指在特定任务的数据集上对预训练模型进行进一步的训练，<<SEP>>以使其更好地适应该任务。<<SEP>>BERT的灵活性使得它可以应用于多种自然语言处理任务，<<SEP>>如文本分类、命名实体识别、问答等。<<SEP>>
6、表现与影响<<SEP>>
BERT在各种自然语言处理任务中都取得了显著的成绩，<<SEP>>刷新了多项基准测试的记录。<<SEP>>它的成功推动了预训练语言模型的发展，<<SEP>>为后续更多先进模型（如RoBERTa、ALBERT等）的出现奠定了基础。<<SEP>>
二、Transformer<<SEP>>
1、传统RNN网络计算时存在的问题<<SEP>>
1）串联<<SEP>>
导致数据必须从h1-h2-…hm。<<SEP>>数据训练时间变长，<<SEP>>因为需要要等h1的结果出来才能计算h2，<<SEP>>h2结果出来才能计算h3，<<SEP>>最终一个一个计算完得到最终结果。<<SEP>>
2）并行计算效果不好<<SEP>>
也就是不能多台服务器同时训练一个网络，<<SEP>>在CNN中可以把多个卷积核放在多台电脑中训练，<<SEP>>但是在RNN中必须从头到尾的来对模型进行训练，<<SEP>>无法在多台电脑上进行训练。<<SEP>>